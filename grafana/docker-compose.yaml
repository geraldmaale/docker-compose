---
version: '3.9'
x-default-logging: &logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"

networks:
  default:
    name: grafana-opentel
    driver: bridge

services:

  read:
    image: grafana/loki:2.7.3
    container_name: loki-read
    command: "-config.file=/etc/loki/config.yaml -target=read"
    restart: always
    ports:
      - 3101:3100
      - 7946
      - 9095
    volumes:
      - ./loki-config.yaml:/etc/loki/config.yaml
      # - grafana-opentel-data:/etc/loki/config.yaml
    depends_on:
      - minio
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    # networks:
    #   - grafana-opentel

  write:
    image: grafana/loki:2.7.3
    container_name: loki-write
    command: "-config.file=/etc/loki/config.yaml -target=write"
    restart: always
    ports:
      - 3102:3100
      - 7946
      - 9095
    volumes:
      - ./loki-config.yaml:/etc/loki/config.yaml
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      - minio
    # networks:
    #   - grafana-opentel

  promtail:
    image: grafana/promtail:2.7.3
    container_name: promtail
    volumes:
      - ./promtail-local-config.yaml:/etc/promtail/config.yaml:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: -config.file=/etc/promtail/config.yaml
    restart: always
    depends_on:
      - gateway
    # networks:
    #   - grafana-opentel

  minio:
    image: minio/minio
    container_name: minio
    restart: always
    entrypoint:
      - sh
      - -euc
      - |
        mkdir -p /data/loki-data && \
        mkdir -p /data/loki-ruler && \
        minio server /data
    environment:
      - MINIO_ACCESS_KEY=loki
      - MINIO_SECRET_KEY=supersecret
      - MINIO_PROMETHEUS_AUTH_TYPE=public
      - MINIO_UPDATE=off
    ports:
      - 9000
    volumes:
      - minio-data:/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 15s
      timeout: 20s
      retries: 5
    # networks:
    #   - grafana-opentel

  grafana:
    image: grafana/grafana-enterprise:9.4.7
    container_name: grafana
    restart: always
    environment:
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - "GF_LOG_MODE=console file"
    depends_on:
      - gateway
    entrypoint:
      - sh
      - -euc
      - |
        mkdir -p /etc/grafana/provisioning/datasources
        cat <<EOF > /etc/grafana/provisioning/datasources/ds.yaml
        apiVersion: 1
        datasources:
          - name: Loki
            type: loki
            access: proxy
            url: http://gateway:3100
            jsonData:
              httpHeaderName1: "X-Scope-OrgID"
            secureJsonData:
              httpHeaderValue1: "tenant1"
        EOF
        /run.sh
    ports:
      - "3000:3000"
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    # networks:
    #   - grafana-opentel
    volumes:
      # - ./grafana:/var/lib/grafana
      - grafana-data:/var/lib/grafana
    logging: *logging

  gateway:
    image: nginx:latest
    container_name: gateway
    restart: always
    depends_on:
      - read
      - write
    entrypoint:
      - sh
      - -euc
      - |
        cat <<EOF > /etc/nginx/nginx.conf
        user  nginx;
        worker_processes  5;  ## Default: 1

        events {
          worker_connections   1000;
        }

        http {
          resolver 127.0.0.11;

          server {
            listen             3100;

            location = / {
              return 200 'OK';
              auth_basic off;
            }

            location = /api/prom/push {
              proxy_pass       http://write:3100\$$request_uri;
            }

            location = /api/prom/tail {
              proxy_pass       http://read:3100\$$request_uri;
              proxy_set_header Upgrade \$$http_upgrade;
              proxy_set_header Connection "upgrade";
            }

            location ~ /api/prom/.* {
              proxy_pass       http://read:3100\$$request_uri;
            }

            location = /loki/api/v1/push {
              proxy_pass       http://write:3100\$$request_uri;
            }

            location = /loki/api/v1/tail {
              proxy_pass       http://read:3100\$$request_uri;
              proxy_set_header Upgrade \$$http_upgrade;
              proxy_set_header Connection "upgrade";
            }

            location ~ /loki/api/.* {
              proxy_pass       http://read:3100\$$request_uri;
            }
          }
        }
        EOF
        /docker-entrypoint.sh nginx -g "daemon off;"
    ports:
      - "3100:3100"
    healthcheck:
      test: ["CMD", "service", "nginx", "status"]
      interval: 10s
      timeout: 5s
      retries: 5
    # networks:
    #   - grafana-opentel

  flog:
    image: mingrammer/flog
    container_name: flog
    restart: always
    command: -f json -d 1s -l
    # networks:
    #   - grafana-opentel

  jaeger:
    image: jaegertracing/all-in-one
    container_name: jaeger
    restart: always
    command:
      - "--memory.max-traces"
      - "10000"
      - "--query.base-path"
      - "/jaeger/ui"
      - "--prometheus.server-url"
      - "http://prometheus:9090"
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
    logging: *logging
    deploy:
      resources:
        limits:
          memory: 300M
    ports:
      - "4317"          # OTLP over gRPC receiver
      - "4318:4318"     # OTLP over HTTP receiver
      - "9464"          # Prometheus exporter
      - "8888"          # metrics endpoint
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "16685:16685"
      - "14250:14250"
      - "14268:14268"
      - "14269:14269"
      - "9411:9411"
    # networks:
    #   - grafana-opentel

  # zipkin:
  #   image: openzipkin/zipkin
  #   container_name: zipkin
  #   restart: always
  #   ports:
  #     - "9411:9411"
  #   networks:
  #     - grafana-opentel

  prometheus:
    image: quay.io/prometheus/prometheus
    # image: prom/prometheus
    container_name: prometheus
    restart: always
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --enable-feature=exemplar-storage
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    logging: *logging
    deploy:
      resources:
        limits:
          memory: 300M
    # networks:
    #   - grafana-opentel

  otelcol:
    image: otel/opentelemetry-collector-contrib
    container_name: otel-col
    restart: always
    deploy:
      resources:
        limits:
          memory: 125M
    command: ["--config=/etc/otel-collector-config.yaml"]
    logging: *logging
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "1888:1888"   # pprof extension
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      # - "4317:4317"   # OTLP gRPC receiver
      # - "4318:4318"   # OTLP http receiver
      - "55679:55679" # zpages extension
    depends_on:
      - jaeger
    # networks:
    #   - grafana-opentel

volumes:
  minio-data:
  grafana-data:
